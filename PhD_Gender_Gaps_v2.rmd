---
title: "Gender Gaps in PhD Graduates"
author: "Xu Dong"
date: "February 21, 2018"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE)
```

## PhD jobs by majors

This is an R Markdown document.

```{r cars}

# Load packages
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)

# Indeed Search Words
# job_title <- "\"Data+Scientist\""
# location <- "Chicago%2C+IL"
location <- "Nationwide"
BASE_URL <- "https://www.indeed.com"


ADV_URL <- paste0('https://www.indeed.com/jobs?as_and=PhD&as_not=&as_cmp=&jt=all&st=&salary=&sr=directhire&radius=25&fromage=any&limit=50&sort=date&psf=advsrch&as_any=&as_phr=&l=', location)


# get the html file from search url
start_page <- read_html(ADV_URL)

# get the total job count 
job_count <- unlist(strsplit(start_page %>% 
                               html_node("#searchCount") %>%
                               html_text(), split = ' ')) 
job_count <- as.numeric(str_replace_all(job_count[length(job_count)-1],',',''))
cat('Total job count: ', job_count)

# Get start page job URLs
links <- start_page %>%
  html_nodes("h2 a") %>%
  html_attr('href')

# Get result page links
page.links <- start_page %>%
  html_nodes(xpath = '//div[contains(@class,"pagination")]//a') %>%
  html_attr('href')

KEYWORDS <- c('Psychology', 'Education', 'Public Administration', 'English', 'Foreign Languages', 'Computer Science','Physics','Business','Agriculture','Architecture','Music','Health','Journalism','Engineering','Mathematics','Communications', 'Statistics', 'History','Math','Social Sciences','Art')


# Clean the raw html - removing commas, tabs, line changers, etc  
clean.text <- function(text)
{
  str_replace_all(text, regex('\r\n|\n|\t|\r|,|/|<|>|\\.'), ' ')
}

# Given running total dataframe and links to scrape skills and compute running total
ScrapeJobLinks <- function(res, job.links){
  for(i in 1:length(job.links)){
    job.url <- paste0(BASE_URL,job.links[i])
    
    # Sys.sleep(1)
    # cat(paste0('Reading job ', i, '\n'))
    
    tryCatch({
      html <- read_html(job.url)
      text <- html_text(html)
      text <- clean.text(text)
      df <- data.frame(skill = KEYWORDS, count = ifelse(str_detect(text, KEYWORDS), 1, 0))
      res$running$count <- res$running$count + df$count
      res$num_jobs <- res$num_jobs + 1
    }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
    # cat(paste0('job.links\n'))
  }
  
  return(res)
}

# For display purpose, we also need the \\b removed from the keyword set
KEYWORDS_DISPLAY <- KEYWORDS

# Create running total dataframe
running <- data.frame(skill = KEYWORDS_DISPLAY, count = rep(0, length(KEYWORDS_DISPLAY)))

# Since the indeed only display max of 20 pages from search result, we cannot use job_count but need to track by creating a num_jobs
num_jobs <- 0

# Here is our results object that contains the two stats
results <- list("running" = running, "num_jobs" = num_jobs)

if(job_count != 0){
  cat('Scraping jobs in Start Page\n')
  results <- ScrapeJobLinks(results, links)
}

# for(p in 1:(length(page.links)-1)){
for(p in 1:3){ 
  
  cat('Moving to Next 50 jobs\n')
  
  # Navigate to next page
  new.page <- read_html(paste0(BASE_URL, page.links[p]))
  
  # Get new page job URLs
  links <- new.page %>%
    html_nodes("h2 a") %>%
    html_attr('href')
  
  # Scrap job links
  results <- ScrapeJobLinks(results, links)
}

print(arrange(results$running, -count))
```

## Job locations for PhDs

```{r map}
library(datasets)
library(googleVis)
suppressPackageStartupMessages(library(googleVis))
states <- data.frame(state.name, state.x77)
GeoStates <- gvisGeoChart(states, "state.name", "Illiteracy",
                          options=list(region="US", 
                                       displayMode="regions", 
                                       resolution="provinces",
                                       width=600, height=400))
plot(GeoStates)
```

